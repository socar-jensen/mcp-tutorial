{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfda5ce-4f4a-4582-b4ae-ab309696eedf",
   "metadata": {},
   "source": [
    "# Lesson 5: Creating an MCP Client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211ebe3-496b-4f54-b811-1b778787ae8f",
   "metadata": {},
   "source": [
    "이전 레슨에서는 2개의 tool을 노출하는 MCP research server를 생성했습니다. 이 레슨에서는 chatbot이 MCP client를 통해 server와 통신하도록 만들 것입니다. 이렇게 하면 chatbot이 MCP compatible하게 됩니다. 레슨 4에서 중단한 지점부터 계속할 것입니다. 즉, `research_server.py` file이 포함된 `mcp_project` folder가 다시 제공됩니다. 여기에 MCP chatbot file을 추가하고 environment를 업데이트할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dcfb4-b58c-4ed5-b63f-d6a1b03642d8",
   "metadata": {},
   "source": [
    "## Back to the Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa381b7c-5203-4795-a726-bd7698deaf62",
   "metadata": {},
   "source": [
    "다음은 레슨 3의 chatbot 예제에서 가져온 주요 code 부분들(`process_query`, `chat_loop`)입니다. tool 정의와 실행의 부담이 이제 MCP server로 이전되었으므로, chatbot logic에는 사용자 query 처리와 사용자가 `quit`을 입력할 때까지 chat loop을 계속 실행하는 것과 관련된 code만 포함되어 있다는 점을 주목하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2e6fe7-a727-402b-bb6e-265bf2a580e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:21:15.618752Z",
     "start_time": "2025-09-25T17:21:14.694283Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API key from environment variable\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', '')\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "def process_query(query):\n",
    "    config = types.GenerateContentConfig(tools=[tools])\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=query,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        \n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                print(part.text)\n",
    "                process_query = False\n",
    "                break\n",
    "            \n",
    "            elif hasattr(part, 'function_call') and part.function_call:\n",
    "                \n",
    "                function_call = part.function_call\n",
    "                tool_name = function_call.name\n",
    "                tool_args = dict(function_call.args)\n",
    "                \n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                \n",
    "                # Continue conversation with function response\n",
    "                function_response = types.FunctionResponse(\n",
    "                    name=tool_name, \n",
    "                    response={\"result\": result}\n",
    "                )\n",
    "                \n",
    "                response = client.models.generate_content(\n",
    "                    model='gemini-2.5-flash',\n",
    "                    contents=[\n",
    "                        types.Content(role='user', parts=[types.Part(text=query)]),\n",
    "                        types.Content(role='model', parts=[types.Part(function_call=function_call)]),\n",
    "                        types.Content(role='function', parts=[types.Part(function_response=function_response)])\n",
    "                    ],\n",
    "                    config=config\n",
    "                )\n",
    "                \n",
    "                # Print the final response\n",
    "                if response.candidates[0].content.parts:\n",
    "                    for final_part in response.candidates[0].content.parts:\n",
    "                        if hasattr(final_part, 'text') and final_part.text:\n",
    "                            print(final_part.text)\n",
    "                            break\n",
    "                process_query = False\n",
    "\n",
    "\n",
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764075a-1ca5-4820-8fdb-6c244a7a8324",
   "metadata": {},
   "source": [
    "## Building your MCP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c61911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b159c4-e06e-4a0c-acb4-028e2046ed73",
   "metadata": {},
   "source": [
    "이제 `process_query`와 `chat_loop` function들을 가져와서 `MCP_ChatBot` class로 래핑할 것입니다. chatbot이 server와 통신할 수 있도록 하기 위해, MCP client를 통해 server에 연결하는 method를 추가할 것입니다. 이는 다음 reference code에 제시된 구조를 따릅니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2225f-f389-4fe6-85c7-6e96aa5bbb18",
   "metadata": {},
   "source": [
    "### Reference Code\n",
    "``` python\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uv\",  # Executable\n",
    "    args=[\"run example_server.py\"],  # Command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    # Launch the server as a subprocess & returns the read and write streams\n",
    "    # read: the stream that the client will use to read msgs from the server\n",
    "    # write: the stream that client will use to write msgs to the server\n",
    "    async with stdio_client(server_params) as (read, write): \n",
    "        # the client session is used to initiate the connection \n",
    "        # and send requests to server \n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection (1:1 connection with the server)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List available tools\n",
    "            tools = await session.list_tools()\n",
    "\n",
    "            # will call the chat_loop here\n",
    "            # ....\n",
    "            \n",
    "            # Call a tool: this will be in the process_query method\n",
    "            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run())\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380fcdc-1703-40a6-9dd0-600c6ba2bdb1",
   "metadata": {},
   "source": [
    "### Adding MCP Client to the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b442185-9c23-47a2-8a7e-f92928a45bed",
   "metadata": {},
   "source": [
    "MCP_ChatBot class는 다음 method들로 구성됩니다:\n",
    "- `process_query`\n",
    "- `chat_loop`\n",
    "- `connect_to_server_and_run`\n",
    "  \n",
    "그리고 다음 attribute들을 가집니다:\n",
    "- session (ClientSession type)\n",
    "- anthropic: Anthropic                           \n",
    "- available_tools\n",
    "\n",
    "`connect_to_server_and_run`에서 client는 server를 실행하고 server가 제공하는 tool들의 list를 요청합니다(client session을 통해). tool 정의들은 `available_tools` variable에 저장되고 `process_query`에서 LLM으로 전달됩니다.\n",
    "\n",
    "<img src=\"images/tools_discovery.png\" width=\"400\">\n",
    "\n",
    "\n",
    "`process_query`에서 LLM이 tool 실행이 필요하다고 판단하면, client session이 server에 tool call request를 전송합니다. 반환된 response는 LLM으로 전달됩니다. \n",
    "\n",
    "<img src=\"images/tool_invocation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96d918-1854-49b5-8930-f1a6962a3ee9",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c305bb8-6474-420e-82b4-e6daecb65d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:31:23.371735Z",
     "start_time": "2025-09-25T17:31:23.368602Z"
    }
   },
   "source": [
    "%%writefile mcp_project/mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API key from environment variable\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', '')\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.client = genai.Client()\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    def build_function_declarations(self, mcp_tools):\n",
    "        \"\"\"Convert MCP tool metadata (JSON Schema) into Gemini FunctionDeclaration objects.\"\"\"\n",
    "        function_declarations = []\n",
    "        for tool in mcp_tools:\n",
    "            schema = tool.get(\"input_schema\", {})\n",
    "            if hasattr(schema, \"model_dump\"):\n",
    "                schema = schema.model_dump()\n",
    "            fd = types.FunctionDeclaration(\n",
    "                name=tool[\"name\"],\n",
    "                description=tool.get(\"description\", \"\"),\n",
    "                parameters=schema or None,\n",
    "            )\n",
    "            function_declarations.append(fd)\n",
    "        return function_declarations\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        # Build Gemini tool configuration from available MCP tools\n",
    "        function_declarations = self.build_function_declarations(self.available_tools)\n",
    "        if function_declarations:\n",
    "            genai_tool = types.Tool(function_declarations=function_declarations)\n",
    "            config = types.GenerateContentConfig(tools=[genai_tool])\n",
    "        else:\n",
    "            config = types.GenerateContentConfig()\n",
    "\n",
    "        response = self.client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=query,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "\n",
    "            for part in response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'text') and part.text:\n",
    "                    print(part.text)\n",
    "                    process_query = False\n",
    "                    break\n",
    "\n",
    "                elif hasattr(part, 'function_call') and part.function_call:\n",
    "\n",
    "                    function_call = part.function_call\n",
    "                    tool_name = function_call.name\n",
    "                    tool_args = dict(function_call.args)\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "\n",
    "                    # Call tool via MCP session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "\n",
    "                    # Extract textual content from MCP tool result\n",
    "                    result_text_parts = []\n",
    "                    for result_part in getattr(result, \"content\", []) or []:\n",
    "                        text_val = getattr(result_part, \"text\", None)\n",
    "                        if text_val:\n",
    "                            result_text_parts.append(text_val)\n",
    "                    result_text = \"\\n\".join(result_text_parts) if result_text_parts else str(getattr(result, \"content\", \"\"))\n",
    "\n",
    "                    # Continue conversation with function response\n",
    "                    function_response = types.FunctionResponse(\n",
    "                        name=tool_name,\n",
    "                        response={\"result\": result_text}\n",
    "                    )\n",
    "\n",
    "                    response = self.client.models.generate_content(\n",
    "                        model='gemini-2.5-flash',\n",
    "                        contents=[\n",
    "                            types.Content(role='user', parts=[types.Part(text=query)]),\n",
    "                            types.Content(role='model', parts=[types.Part(function_call=function_call)]),\n",
    "                            types.Content(role='function', parts=[types.Part(function_response=function_response)])\n",
    "                        ],\n",
    "                        config=config\n",
    "                    )\n",
    "\n",
    "                    # Print the final response\n",
    "                    if response.candidates[0].content.parts:\n",
    "                        for final_part in response.candidates[0].content.parts:\n",
    "                            if hasattr(final_part, 'text') and final_part.text:\n",
    "                                print(final_part.text)\n",
    "                                break\n",
    "                    process_query = False\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "\n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection\n",
    "                await session.initialize()\n",
    "\n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "\n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "\n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "\n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "2f08496f",
   "metadata": {},
   "source": [
    "### 수정된 Chatbot 코드 요약\n",
    "- mcp_types.Tool 사용 제거: MCP 프로토콜의 Tool 모델( name, inputSchema 필수 ) 대신 Google Generative AI SDK의 `types.Tool` 사용\n",
    "- MCP 서버 tool 메타데이터(JSON Schema)를 Gemini `types.FunctionDeclaration` 리스트로 변환하는 `build_function_declarations` 메서드 추가\n",
    "- `process_query` 에서 function_declarations 존재 시 `types.Tool(function_declarations=[...])` 구성하여 `GenerateContentConfig` 에 전달\n",
    "- Tool 실행 결과(`session.call_tool`)의 `result.content` 파트들에서 `.text` 추출 후 LLM function response로 전달\n",
    "- 결과 문자열 합성 로직 추가(여러 part 대비)\n",
    "\n",
    "이제 validation 오류(name, inputSchema missing)는 발생하지 않아야 하며, LLM이 function_call 을 생성하면 MCP 서버 tool이 호출되고 응답이 모델의 후속 출력에 반영됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df7af0-162a-4419-a188-12a2c6058ab2",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833b436ceaefc5d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:25:25.998324Z",
     "start_time": "2025-09-25T17:25:25.988717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_project/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/main.py\n",
    "\n",
    "def main():\n",
    "    print(\"Hello from mcp-project!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0cfcf6c5d0ffcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:25:27.726026Z",
     "start_time": "2025-09-25T17:25:27.722609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_project/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/pyproject.toml\n",
    "[project]\n",
    "name = \"mcp-project\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Add your description here\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.11\"\n",
    "dependencies = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca4737-0d67-4429-9379-8228b5c9ebf1",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d511872f-9c1e-481a-a3b1-0069f3588981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run the MCP chatbot locally:\n",
      "1. Open your system terminal\n",
      "2. Navigate to the mcp_project directory: cd mcp_project\n",
      "3. Activate virtual environment: source .venv/bin/activate\n",
      "4. Install additional dependencies: uv add google-generativeai python-dotenv nest_asyncio\n",
      "5. Run the chatbot: uv run mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "# For local development, use your system terminal\n",
    "print(\"To run the MCP chatbot locally:\")\n",
    "print(\"1. Open your system terminal\")\n",
    "print(\"2. Navigate to the mcp_project directory: cd mcp_project\")\n",
    "print(\"3. Activate virtual environment: source .venv/bin/activate\")  \n",
    "print(\"4. Install additional dependencies: uv add google-generativeai python-dotenv nest_asyncio\")\n",
    "print(\"5. Run the chatbot: uv run mcp_chatbot.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-chatbot-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
