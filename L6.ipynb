{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc712d2-8b62-4098-a986-76ec60c795c4",
   "metadata": {},
   "source": [
    "# Lesson 6: Connecting the MCP Chatbot to Reference Servers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c374c1-4691-4aed-a145-4b46167ff197",
   "metadata": {},
   "source": [
    "이제 MCP chatbot이 모든 MCP server에 연결할 수 있도록 하여 chatbot 기능을 확장할 것입니다. 이전 레슨에서 구축한 research server의 tool들에 더해, 두 개의 공식 MCP server tool들을 통합할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd0f0-ffdf-4124-ad5a-d734403eb020",
   "metadata": {},
   "source": [
    "<img src=\"images/lesson_6.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216c450-075a-426f-907f-e0fff64df6c5",
   "metadata": {},
   "source": [
    "## Open-Source MCP Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495f41b-301b-4243-b5ab-724407bc338a",
   "metadata": {},
   "source": [
    "이 [repo](https://github.com/modelcontextprotocol/servers)에서 MCP server들의 reference implementation 모음뿐만 아니라 community에서 구축한 server들에 대한 reference와 추가 resource들을 찾을 수 있습니다. 두 개의 reference server를 사용하여 MCP chatbot에 해당 tool들을 통합할 것입니다:\n",
    "- [fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch): 인터넷에서 URL을 fetch하고 내용을 markdown으로 추출하는 `fetch` tool을 제공합니다.\n",
    "- [filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem): 지정한 directory 내의 file들과 directory들과 상호작용하기 위한 여러 tool들을 제공합니다.\n",
    "\n",
    "각 server가 노출하는 기능들과 실행 방법을 확인하려면 각 server의 readme file을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f68ad1-d038-4e31-809e-3704a4433d1b",
   "metadata": {},
   "source": [
    "## Updating the MCP Chatbot - Optional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f25f1-5ee2-4cff-b1b1-fc5db7145a62",
   "metadata": {},
   "source": [
    "Here are the updates you'll make to the chatbot. You're encouraged to read this section before or after you watch the video, if you'd like to learn more about the details of the code.\n",
    "\n",
    "- chatbot에서 server parameter들을 하드코딩하는 대신, chatbot이 JSON file에서 server configuration들을 읽어올 것입니다:\n",
    "  ### Server Configuration\n",
    "  In the `L6/mcp_project`, you can find the `server_config.json` configuration file that has the following structure.\n",
    "    ``` json\n",
    "    {\n",
    "        \"mcpServers\": {\n",
    "            \n",
    "            \"filesystem\": {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\n",
    "                    \"-y\",\n",
    "                    \"@modelcontextprotocol/server-filesystem\",\n",
    "                    \".\"\n",
    "                ]\n",
    "            },\n",
    "            \n",
    "            \"research\": {\n",
    "                \"command\": \"uv\",\n",
    "                \"args\": [\"run\", \"research_server.py\"]\n",
    "            },\n",
    "            \n",
    "            \"fetch\": {\n",
    "                \"command\": \"uvx\",\n",
    "                \"args\": [\"mcp-server-fetch\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "reference server들의 경우, `npx`와 `uvx` command들이 server file들을 로컬 environment에 직접 설치합니다(미리 설치할 필요가 없습니다). `filesystem`의 경우 `.`이 세 번째 argument로 제공되며 이는 \"현재 directory\"를 의미합니다. 즉, `fetch` server가 현재 directory 내의 file들과 directory들과 상호작용할 수 있도록 허용하는 것입니다."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:33:27.454258Z",
     "start_time": "2025-09-25T17:33:27.450683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile mcp_project/server_config.json\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@modelcontextprotocol/server-filesystem\",\n",
    "                \".\"\n",
    "            ]\n",
    "        },\n",
    "\n",
    "        \"research\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\"run\", \"research_server.py\"]\n",
    "        },\n",
    "\n",
    "        \"fetch\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\"mcp-server-fetch\"]\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "8b5c52232110c9e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_project/server_config.json\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d8d38af9-7777-4f9e-b314-e882e2b7ca8c",
   "metadata": {},
   "source": [
    "- 다음은 업데이트된 MCP_chatbot의 대략적인 diagram입니다:\n",
    "  \n",
    "  <img src=\"images/updated_class.png\" width=\"600\">\n",
    "\n",
    "  1. 하나의 session을 갖는 대신, 이제 각 client session이 각 server와 1대1 연결을 설정하는 client session들의 list를 가집니다.\n",
    "  2. `available_tools`에는 chatbot이 연결할 수 있는 모든 server들에 의해 노출되는 모든 tool들의 정의가 포함됩니다.\n",
    "  3. `tool_to_session`은 tool 이름을 해당 client session에 mapping합니다. 이렇게 하면 LLM이 특정 tool 이름을 결정할 때, 올바른 client session에 mapping하여 해당 session을 사용해 올바른 MCP server에 `tool_call` request를 보낼 수 있습니다.\n",
    "  4. `exit_stack`은 mcp client object들과 그들의 session들을 관리하고 적절히 닫힌다는 것을 보장하는 context manager입니다. 레슨 5에서는 `with` statement를 사용했기 때문에 이를 사용하지 않았습니다. `with` statement는 내부적으로 context manager를 사용합니다. 여기서도 다시 `with` statement를 사용할 수 있지만, 연결할 server가 여러 개이므로 여러 개의 중첩된 `with` statement를 사용하게 될 수 있습니다. `exit_stack`을 사용하면 아래 code에서 보게 될 것처럼 mcp client들과 그들의 session들을 동적으로 추가할 수 있습니다.\n",
    "  5. `connect_to_servers`는 server configuration file을 읽고 각 개별 server에 대해 helper method인 `connect_to_server`를 호출합니다. 이 후자 method에서는 MCP client가 생성되어 server를 sub-process로 실행하는 데 사용되고, 그 다음 client session이 생성되어 server에 연결하고 server가 제공하는 tool들의 list 설명을 가져옵니다.\n",
    "  6. `cleanup`은 모든 connection들이 작업을 마쳤을 때 적절히 종료되도록 보장하는 helper method입니다. 레슨 5에서는 resource를 자동으로 정리하기 위해 `with` statement에 의존했습니다. 이 cleanup method는 비슷한 목적을 제공하지만, exit_stack에 추가한 모든 resource들에 대해서입니다. 이는 접시를 쌓고 언스택하는 것처럼 추가된 역순으로 (MCP client들과 session들을) 닫습니다. 이는 resource leak을 피하기 위해 network programming에서 특히 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7159b5-02ba-413a-b30c-4f7725379c46",
   "metadata": {},
   "source": [
    "## Updated Code for the MCP Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ee81457-ab55-4acb-9dc6-6b4443344d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T17:39:46.458008Z",
     "start_time": "2025-09-25T17:39:46.453683Z"
    }
   },
   "source": [
    "%%writefile mcp_project/mcp_chatbot.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set the API key from environment variable\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', '')\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.client = genai.Client()\n",
    "        self.chat = None  # Chat session for maintaining context\n",
    "        self.available_tools: List[ToolDefinition] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "\n",
    "    def simplify_schema_for_genai(self, schema):\n",
    "        \"\"\"Convert MCP schema to a simpler format that Google genai accepts\"\"\"\n",
    "        if isinstance(schema, dict):\n",
    "            simplified = {\"type\": \"object\", \"properties\": {}}\n",
    "\n",
    "            # Handle the root object\n",
    "            if \"properties\" in schema:\n",
    "                for prop_name, prop_def in schema[\"properties\"].items():\n",
    "                    if isinstance(prop_def, dict):\n",
    "                        prop_type = prop_def.get(\"type\", \"string\")\n",
    "                        simplified_prop = {\"type\": prop_type}\n",
    "\n",
    "                        if \"description\" in prop_def:\n",
    "                            simplified_prop[\"description\"] = prop_def[\"description\"]\n",
    "\n",
    "                        # Handle array types simply\n",
    "                        if prop_type == \"array\" and \"items\" in prop_def:\n",
    "                            items_type = prop_def[\"items\"].get(\"type\", \"string\")\n",
    "                            simplified_prop[\"items\"] = {\"type\": items_type}\n",
    "\n",
    "                        simplified[\"properties\"][prop_name] = simplified_prop\n",
    "\n",
    "            # Add required fields if they exist\n",
    "            if \"required\" in schema:\n",
    "                simplified[\"required\"] = schema[\"required\"]\n",
    "\n",
    "            return simplified\n",
    "        else:\n",
    "            return schema\n",
    "\n",
    "    def convert_mcp_tools_to_genai_format(self, mcp_tools):\n",
    "        \"\"\"Convert MCP tools to Gemini format\"\"\"\n",
    "        gemini_tools = []\n",
    "        for tool in mcp_tools:\n",
    "            # Simplify the input schema for Google genai compatibility\n",
    "            simplified_schema = self.simplify_schema_for_genai(tool[\"input_schema\"])\n",
    "\n",
    "            gemini_tool = {\n",
    "                \"name\": tool[\"name\"],\n",
    "                \"description\": tool[\"description\"],\n",
    "                \"parameters\": simplified_schema\n",
    "            }\n",
    "            gemini_tools.append(gemini_tool)\n",
    "        return gemini_tools\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
    "        \"\"\"Connect to a single MCP server.\"\"\"\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )\n",
    "            await session.initialize()\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            # List available tools for this session\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for tool in tools:\n",
    "                self.tool_to_session[tool.name] = session\n",
    "                self.available_tools.append({\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):\n",
    "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "\n",
    "            # Initialize chat session after connecting to servers\n",
    "            await self.initialize_chat()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def initialize_chat(self):\n",
    "        \"\"\"Initialize the chat session with available tools.\"\"\"\n",
    "        gemini_tools = self.convert_mcp_tools_to_genai_format(self.available_tools)\n",
    "        tools_config = types.Tool(function_declarations=gemini_tools) if gemini_tools else None\n",
    "\n",
    "        config = types.GenerateContentConfig(tools=[tools_config]) if tools_config else types.GenerateContentConfig()\n",
    "\n",
    "        self.chat = self.client.chats.create(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        # Send message to chat session\n",
    "        response = self.chat.send_message(query)\n",
    "\n",
    "        while True:\n",
    "            # Check if response has function calls\n",
    "            has_function_call = False\n",
    "            has_text_response = False\n",
    "\n",
    "            for part in response.candidates[0].content.parts:\n",
    "                if hasattr(part, 'function_call') and part.function_call:\n",
    "                    has_function_call = True\n",
    "                    function_call = part.function_call\n",
    "                    tool_name = function_call.name\n",
    "                    tool_args = dict(function_call.args)\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "\n",
    "                    # Call tool via correct MCP session\n",
    "                    session = self.tool_to_session[tool_name]\n",
    "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
    "\n",
    "                    # Send function response to continue conversation\n",
    "                    function_response = types.FunctionResponse(\n",
    "                        name=tool_name,\n",
    "                        response={\"result\": str(result.content)}\n",
    "                    )\n",
    "\n",
    "                    response = self.chat.send_message(\n",
    "                        types.Part(function_response=function_response)\n",
    "                    )\n",
    "                    break  # Process one function call at a time\n",
    "\n",
    "                elif hasattr(part, 'text') and part.text:\n",
    "                    has_text_response = True\n",
    "                    print(part.text)\n",
    "\n",
    "            # If no function calls, we're done\n",
    "            if not has_function_call:\n",
    "                break\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "\n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers()\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "6f221f83-6e8b-431e-bd27-6dccedfac408",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21edb4-aba4-4b5a-b3c6-8aa908a7c8df",
   "metadata": {},
   "source": [
    "chatbot과 상호작용해 보세요. 다음은 query 예시들입니다:\n",
    "- Fetch the content of this website: https://modelcontextprotocol.io/docs/concepts/architecture and save the content in the file \"mcp_summary.md\", create a visual diagram that summarizes the content of \"mcp_summary.md\" and save it in a text file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-chatbot-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
